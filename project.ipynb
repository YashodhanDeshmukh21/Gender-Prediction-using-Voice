{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13724359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tqdm\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "label2int = {\n",
    "    \"male\": 1,\n",
    "    \"female\": 0\n",
    "}\n",
    "\n",
    "\n",
    "def load_data(vector_length=128):\n",
    "    \"\"\"A function to load gender recognition dataset from `data` folder\n",
    "    After the second run, this will load from results/features.npy and results/labels.npy files\n",
    "    as it is much faster!\"\"\"\n",
    "    # make sure results folder exists\n",
    "    if not os.path.isdir(\"results\"):\n",
    "        os.mkdir(\"results\")\n",
    "    # if features & labels already loaded individually and bundled, load them from there instead\n",
    "    if os.path.isfile(\"results/features.npy\") and os.path.isfile(\"results/labels.npy\"):\n",
    "        X = np.load(\"results/features.npy\")\n",
    "        y = np.load(\"results/labels.npy\")\n",
    "        return X, y\n",
    "    # read dataframe\n",
    "    df = pd.read_csv(\"balanced-all.csv\")\n",
    "    # get total samples\n",
    "    n_samples = len(df)\n",
    "    # get total male samples\n",
    "    n_male_samples = len(df[df['gender'] == 'male'])\n",
    "    # get total female samples\n",
    "    n_female_samples = len(df[df['gender'] == 'female'])\n",
    "    print(\"Total samples:\", n_samples)\n",
    "    print(\"Total male samples:\", n_male_samples)\n",
    "    print(\"Total female samples:\", n_female_samples)\n",
    "    # initialize an empty array for all audio features\n",
    "    X = np.zeros((n_samples, vector_length))\n",
    "    # initialize an empty array for all audio labels (1 for male and 0 for female)\n",
    "    y = np.zeros((n_samples, 1))\n",
    "    for i, (filename, gender) in tqdm.tqdm(enumerate(zip(df['filename'], df['gender'])), \"Loading data\", total=n_samples):\n",
    "        features = np.load(filename)\n",
    "        X[i] = features\n",
    "        y[i] = label2int[gender]\n",
    "    # save the audio features and labels into files\n",
    "    # so we won't load each one of them next run\n",
    "    np.save(\"results/features\", X)\n",
    "    np.save(\"results/labels\", y)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def split_data(X, y, test_size=0.1, valid_size=0.1):\n",
    "    # split training set and testing set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=7)\n",
    "    # split training set and validation set\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=valid_size, random_state=7)\n",
    "    # return a dictionary of values\n",
    "    return {\n",
    "        \"X_train\": X_train,\n",
    "        \"X_valid\": X_valid,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_train\": y_train,\n",
    "        \"y_valid\": y_valid,\n",
    "        \"y_test\": y_test\n",
    "    }\n",
    "\n",
    "\n",
    "def create_model(vector_length=128):\n",
    "    \"\"\"5 hidden dense layers from 256 units to 64, not the best model, but not bad.\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_shape=(vector_length,)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    # one output neuron with sigmoid activation function, 0 means female, 1 means male\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    # using binary crossentropy as it's male/female classification (binary)\n",
    "    model.compile(loss=\"binary_crossentropy\", metrics=[\"accuracy\"], optimizer=\"adam\")\n",
    "    # print summary of the model\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfd008d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 156,545\n",
      "Trainable params: 156,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "848/848 [==============================] - 6s 5ms/step - loss: 0.5496 - accuracy: 0.7756 - val_loss: 0.3608 - val_accuracy: 0.8543\n",
      "Epoch 2/100\n",
      "848/848 [==============================] - 4s 5ms/step - loss: 0.4199 - accuracy: 0.8344 - val_loss: 0.3427 - val_accuracy: 0.8684\n",
      "Epoch 3/100\n",
      "848/848 [==============================] - 4s 5ms/step - loss: 0.3809 - accuracy: 0.8515 - val_loss: 0.3107 - val_accuracy: 0.8759\n",
      "Epoch 4/100\n",
      "848/848 [==============================] - 4s 5ms/step - loss: 0.3569 - accuracy: 0.8635 - val_loss: 0.3118 - val_accuracy: 0.8812\n",
      "Epoch 5/100\n",
      "848/848 [==============================] - 4s 5ms/step - loss: 0.3451 - accuracy: 0.8674 - val_loss: 0.2887 - val_accuracy: 0.8906\n",
      "Epoch 6/100\n",
      "848/848 [==============================] - 4s 5ms/step - loss: 0.3321 - accuracy: 0.8732 - val_loss: 0.2927 - val_accuracy: 0.8827\n",
      "Epoch 7/100\n",
      "848/848 [==============================] - 4s 5ms/step - loss: 0.3275 - accuracy: 0.8774 - val_loss: 0.2801 - val_accuracy: 0.8916\n",
      "Epoch 8/100\n",
      "848/848 [==============================] - 4s 5ms/step - loss: 0.3160 - accuracy: 0.8791 - val_loss: 0.2776 - val_accuracy: 0.8923\n",
      "Epoch 9/100\n",
      "848/848 [==============================] - 4s 5ms/step - loss: 0.3130 - accuracy: 0.8814 - val_loss: 0.2695 - val_accuracy: 0.8966\n",
      "Epoch 10/100\n",
      "848/848 [==============================] - 4s 5ms/step - loss: 0.3004 - accuracy: 0.8863 - val_loss: 0.2682 - val_accuracy: 0.8954\n",
      "Epoch 11/100\n",
      "848/848 [==============================] - 4s 5ms/step - loss: 0.3011 - accuracy: 0.8850 - val_loss: 0.2667 - val_accuracy: 0.8974\n",
      "Epoch 12/100\n",
      "848/848 [==============================] - 4s 5ms/step - loss: 0.3007 - accuracy: 0.8880 - val_loss: 0.2607 - val_accuracy: 0.9064\n",
      "Epoch 13/100\n",
      "848/848 [==============================] - 4s 5ms/step - loss: 0.2921 - accuracy: 0.8908 - val_loss: 0.2601 - val_accuracy: 0.9047\n",
      "Epoch 14/100\n",
      "848/848 [==============================] - 4s 5ms/step - loss: 0.2898 - accuracy: 0.8906 - val_loss: 0.2625 - val_accuracy: 0.9067\n",
      "Epoch 15/100\n",
      "848/848 [==============================] - 4s 5ms/step - loss: 0.2830 - accuracy: 0.8927 - val_loss: 0.2418 - val_accuracy: 0.9074\n",
      "Epoch 16/100\n",
      "848/848 [==============================] - 4s 5ms/step - loss: 0.2780 - accuracy: 0.8954 - val_loss: 0.2420 - val_accuracy: 0.9061\n",
      "Epoch 17/100\n",
      "848/848 [==============================] - 4s 5ms/step - loss: 0.2787 - accuracy: 0.8944 - val_loss: 0.2486 - val_accuracy: 0.9047\n",
      "Epoch 18/100\n",
      "848/848 [==============================] - 4s 5ms/step - loss: 0.2799 - accuracy: 0.8952 - val_loss: 0.2395 - val_accuracy: 0.9072\n",
      "Epoch 19/100\n",
      "848/848 [==============================] - 5s 5ms/step - loss: 0.2759 - accuracy: 0.8957 - val_loss: 0.2380 - val_accuracy: 0.9159\n",
      "Epoch 20/100\n",
      "848/848 [==============================] - 4s 5ms/step - loss: 0.2794 - accuracy: 0.8974 - val_loss: 0.2399 - val_accuracy: 0.9124\n",
      "Epoch 21/100\n",
      "848/848 [==============================] - 4s 5ms/step - loss: 0.2724 - accuracy: 0.8973 - val_loss: 0.2284 - val_accuracy: 0.9160\n",
      "Epoch 22/100\n",
      "848/848 [==============================] - 4s 5ms/step - loss: 0.2714 - accuracy: 0.8985 - val_loss: 0.2407 - val_accuracy: 0.9132\n",
      "Epoch 23/100\n",
      "848/848 [==============================] - 4s 5ms/step - loss: 0.2675 - accuracy: 0.8999 - val_loss: 0.2393 - val_accuracy: 0.9104\n",
      "Epoch 24/100\n",
      "848/848 [==============================] - 4s 5ms/step - loss: 0.2681 - accuracy: 0.8999 - val_loss: 0.2321 - val_accuracy: 0.9145\n",
      "Epoch 25/100\n",
      "848/848 [==============================] - 4s 5ms/step - loss: 0.2660 - accuracy: 0.9013 - val_loss: 0.2278 - val_accuracy: 0.9152\n",
      "Epoch 26/100\n",
      "848/848 [==============================] - 5s 6ms/step - loss: 0.2663 - accuracy: 0.8992 - val_loss: 0.2294 - val_accuracy: 0.9172\n",
      "Epoch 27/100\n",
      "848/848 [==============================] - 4s 5ms/step - loss: 0.2644 - accuracy: 0.9011 - val_loss: 0.2266 - val_accuracy: 0.9122\n",
      "Epoch 28/100\n",
      "848/848 [==============================] - 4s 5ms/step - loss: 0.2613 - accuracy: 0.9036 - val_loss: 0.2289 - val_accuracy: 0.9163\n",
      "Epoch 29/100\n",
      "848/848 [==============================] - 4s 5ms/step - loss: 0.2589 - accuracy: 0.9043 - val_loss: 0.2276 - val_accuracy: 0.9172\n",
      "Epoch 30/100\n",
      "848/848 [==============================] - 4s 5ms/step - loss: 0.2578 - accuracy: 0.9034 - val_loss: 0.2303 - val_accuracy: 0.9110\n",
      "Epoch 31/100\n",
      "848/848 [==============================] - 4s 5ms/step - loss: 0.2607 - accuracy: 0.9042 - val_loss: 0.2262 - val_accuracy: 0.9180\n",
      "Epoch 32/100\n",
      "848/848 [==============================] - 5s 5ms/step - loss: 0.2567 - accuracy: 0.9043 - val_loss: 0.2256 - val_accuracy: 0.9155\n",
      "Epoch 33/100\n",
      "848/848 [==============================] - 5s 6ms/step - loss: 0.2638 - accuracy: 0.9022 - val_loss: 0.2233 - val_accuracy: 0.9170\n",
      "Epoch 34/100\n",
      "848/848 [==============================] - 5s 5ms/step - loss: 0.2558 - accuracy: 0.9046 - val_loss: 0.2329 - val_accuracy: 0.9144\n",
      "Epoch 35/100\n",
      "848/848 [==============================] - 4s 5ms/step - loss: 0.2526 - accuracy: 0.9050 - val_loss: 0.2247 - val_accuracy: 0.9145\n",
      "Epoch 36/100\n",
      "848/848 [==============================] - 4s 5ms/step - loss: 0.2518 - accuracy: 0.9056 - val_loss: 0.2347 - val_accuracy: 0.9072\n",
      "Epoch 37/100\n",
      "848/848 [==============================] - 5s 5ms/step - loss: 0.2508 - accuracy: 0.9070 - val_loss: 0.2295 - val_accuracy: 0.9125\n",
      "Epoch 38/100\n",
      "848/848 [==============================] - 4s 5ms/step - loss: 0.2502 - accuracy: 0.9076 - val_loss: 0.2180 - val_accuracy: 0.9185\n",
      "Epoch 39/100\n",
      "848/848 [==============================] - 4s 5ms/step - loss: 0.2570 - accuracy: 0.9055 - val_loss: 0.2243 - val_accuracy: 0.9139\n",
      "Epoch 40/100\n",
      "848/848 [==============================] - 4s 5ms/step - loss: 0.2464 - accuracy: 0.9090 - val_loss: 0.2253 - val_accuracy: 0.9192\n",
      "Epoch 41/100\n",
      "848/848 [==============================] - 4s 5ms/step - loss: 0.2461 - accuracy: 0.9082 - val_loss: 0.2246 - val_accuracy: 0.9145\n",
      "Epoch 42/100\n",
      "848/848 [==============================] - 4s 5ms/step - loss: 0.2541 - accuracy: 0.9072 - val_loss: 0.2334 - val_accuracy: 0.9159\n",
      "Epoch 43/100\n",
      "848/848 [==============================] - 4s 5ms/step - loss: 0.2497 - accuracy: 0.9088 - val_loss: 0.2307 - val_accuracy: 0.9145\n",
      "Evaluating the model using 6694 samples...\n",
      "Loss: 0.2210\n",
      "Accuracy: 91.77%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "\n",
    "from utils import load_data, split_data, create_model\n",
    "\n",
    "# load the dataset\n",
    "X, y = load_data()\n",
    "# split the data into training, validation and testing sets\n",
    "data = split_data(X, y, test_size=0.1, valid_size=0.1)\n",
    "# construct the model\n",
    "model = create_model()\n",
    "\n",
    "# use tensorboard to view metrics\n",
    "tensorboard = TensorBoard(log_dir=\"logs\")\n",
    "# define early stopping to stop training after 5 epochs of not improving\n",
    "early_stopping = EarlyStopping(mode=\"min\", patience=5, restore_best_weights=True)\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "\n",
    "# train the model using the training set and validating using validation set\n",
    "model.fit(data[\"X_train\"], data[\"y_train\"], epochs=epochs, batch_size=batch_size, validation_data=(data[\"X_valid\"], data[\"y_valid\"]),\n",
    "          callbacks=[tensorboard, early_stopping])\n",
    "\n",
    "# save the model to a file\n",
    "model.save(\"results/model.h5\")\n",
    "\n",
    "# evaluating the model using the testing set\n",
    "print(f\"Evaluating the model using {len(data['X_test'])} samples...\")\n",
    "loss, accuracy = model.evaluate(data[\"X_test\"], data[\"y_test\"], verbose=0)\n",
    "print(f\"Loss: {loss:.4f}\")\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d9dd31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_24 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 156,545\n",
      "Trainable params: 156,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yasho\\AppData\\Local\\Temp\\ipykernel_18124\\475578842.py:140: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  X, sample_rate = librosa.core.load(file_name)\n"
     ]
    },
    {
     "ename": "NoBackendError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\librosa\\core\\audio.py:176\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 176\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__soundfile_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m sf\u001b[38;5;241m.\u001b[39mSoundFileRuntimeError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;66;03m# If soundfile failed, try audioread instead\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\librosa\\core\\audio.py:209\u001b[0m, in \u001b[0;36m__soundfile_load\u001b[1;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;66;03m# Otherwise, create the soundfile object\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[43msf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context \u001b[38;5;28;01mas\u001b[39;00m sf_desc:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\soundfile.py:658\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[1;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info \u001b[38;5;241m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[0;32m    657\u001b[0m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[1;32m--> 658\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode)\u001b[38;5;241m.\u001b[39missuperset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\soundfile.py:1216\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[1;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[0;32m   1215\u001b[0m     err \u001b[38;5;241m=\u001b[39m _snd\u001b[38;5;241m.\u001b[39msf_error(file_ptr)\n\u001b[1;32m-> 1216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LibsndfileError(err, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError opening \u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode_int \u001b[38;5;241m==\u001b[39m _snd\u001b[38;5;241m.\u001b[39mSFM_WRITE:\n\u001b[0;32m   1218\u001b[0m     \u001b[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[0;32m   1219\u001b[0m     \u001b[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[0;32m   1220\u001b[0m     \u001b[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n",
      "\u001b[1;31mLibsndfileError\u001b[0m: Error opening 'C:\\\\Users\\\\yasho\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-8924deef-d4ce-45a7-b756-b1d2ba0205b7.json': Format not recognised.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNoBackendError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 184\u001b[0m\n\u001b[0;32m    182\u001b[0m     record_to_file(file)\n\u001b[0;32m    183\u001b[0m \u001b[38;5;66;03m# extract features and reshape it\u001b[39;00m\n\u001b[1;32m--> 184\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# predict the gender!\u001b[39;00m\n\u001b[0;32m    186\u001b[0m male_prob \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(features)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[1;32mIn[10], line 140\u001b[0m, in \u001b[0;36mextract_feature\u001b[1;34m(file_name, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m contrast \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontrast\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    139\u001b[0m tonnetz \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtonnetz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 140\u001b[0m X, sample_rate \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chroma \u001b[38;5;129;01mor\u001b[39;00m contrast:\n\u001b[0;32m    142\u001b[0m     stft \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(librosa\u001b[38;5;241m.\u001b[39mstft(X))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\librosa\\core\\audio.py:184\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, (\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPurePath)):\n\u001b[0;32m    181\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    182\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPySoundFile failed. Trying audioread instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    183\u001b[0m     )\n\u001b[1;32m--> 184\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__audioread_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\librosa\\util\\decorators.py:60\u001b[0m, in \u001b[0;36mdeprecated.<locals>.__wrapper\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Warn the user, and then proceed.\"\"\"\u001b[39;00m\n\u001b[0;32m     52\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mDeprecated as of librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mIt will be removed in librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     58\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# Would be 2, but the decorator adds a level\u001b[39;00m\n\u001b[0;32m     59\u001b[0m )\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\librosa\\core\\audio.py:241\u001b[0m, in \u001b[0;36m__audioread_load\u001b[1;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[0;32m    238\u001b[0m     reader \u001b[38;5;241m=\u001b[39m path\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m# If the input was not an audioread object, try to open it\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m     reader \u001b[38;5;241m=\u001b[39m \u001b[43maudioread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m reader \u001b[38;5;28;01mas\u001b[39;00m input_file:\n\u001b[0;32m    244\u001b[0m     sr_native \u001b[38;5;241m=\u001b[39m input_file\u001b[38;5;241m.\u001b[39msamplerate\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\audioread\\__init__.py:132\u001b[0m, in \u001b[0;36maudio_open\u001b[1;34m(path, backends)\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m# All backends failed!\u001b[39;00m\n\u001b[1;32m--> 132\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m NoBackendError()\n",
      "\u001b[1;31mNoBackendError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import os\n",
    "import wave\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sys import byteorder\n",
    "from array import array\n",
    "from struct import pack\n",
    "\n",
    "\n",
    "THRESHOLD = 500\n",
    "CHUNK_SIZE = 1024\n",
    "FORMAT = pyaudio.paInt16\n",
    "RATE = 16000\n",
    "\n",
    "SILENCE = 30\n",
    "\n",
    "def is_silent(snd_data):\n",
    "    \"Returns 'True' if below the 'silent' threshold\"\n",
    "    return max(snd_data) < THRESHOLD\n",
    "\n",
    "def normalize(snd_data):\n",
    "    \"Average the volume out\"\n",
    "    MAXIMUM = 16384\n",
    "    times = float(MAXIMUM)/max(abs(i) for i in snd_data)\n",
    "\n",
    "    r = array('h')\n",
    "    for i in snd_data:\n",
    "        r.append(int(i*times))\n",
    "    return r\n",
    "\n",
    "def trim(snd_data):\n",
    "    \"Trim the blank spots at the start and end\"\n",
    "    def _trim(snd_data):\n",
    "        snd_started = False\n",
    "        r = array('h')\n",
    "\n",
    "        for i in snd_data:\n",
    "            if not snd_started and abs(i)>THRESHOLD:\n",
    "                snd_started = True\n",
    "                r.append(i)\n",
    "\n",
    "            elif snd_started:\n",
    "                r.append(i)\n",
    "        return r\n",
    "\n",
    "    # Trim to the left\n",
    "    snd_data = _trim(snd_data)\n",
    "\n",
    "    # Trim to the right\n",
    "    snd_data.reverse()\n",
    "    snd_data = _trim(snd_data)\n",
    "    snd_data.reverse()\n",
    "    return snd_data\n",
    "\n",
    "def add_silence(snd_data, seconds):\n",
    "    \"Add silence to the start and end of 'snd_data' of length 'seconds' (float)\"\n",
    "    r = array('h', [0 for i in range(int(seconds*RATE))])\n",
    "    r.extend(snd_data)\n",
    "    r.extend([0 for i in range(int(seconds*RATE))])\n",
    "    return r\n",
    "\n",
    "def record():\n",
    "    \"\"\"\n",
    "    Record a word or words from the microphone and \n",
    "    return the data as an array of signed shorts.\n",
    "    Normalizes the audio, trims silence from the \n",
    "    start and end, and pads with 0.5 seconds of \n",
    "    blank sound to make sure VLC et al can play \n",
    "    it without getting chopped off.\n",
    "    \"\"\"\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=FORMAT, channels=1, rate=RATE,\n",
    "        input=True, output=True,\n",
    "        frames_per_buffer=CHUNK_SIZE)\n",
    "\n",
    "    num_silent = 0\n",
    "    snd_started = False\n",
    "\n",
    "    r = array('h')\n",
    "\n",
    "    while 1:\n",
    "        # little endian, signed short\n",
    "        snd_data = array('h', stream.read(CHUNK_SIZE))\n",
    "        if byteorder == 'big':\n",
    "            snd_data.byteswap()\n",
    "        r.extend(snd_data)\n",
    "\n",
    "        silent = is_silent(snd_data)\n",
    "\n",
    "        if silent and snd_started:\n",
    "            num_silent += 1\n",
    "        elif not silent and not snd_started:\n",
    "            snd_started = True\n",
    "\n",
    "        if snd_started and num_silent > SILENCE:\n",
    "            break\n",
    "\n",
    "    sample_width = p.get_sample_size(FORMAT)\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "    r = normalize(r)\n",
    "    r = trim(r)\n",
    "    r = add_silence(r, 0.5)\n",
    "    return sample_width, r\n",
    "\n",
    "def record_to_file(path):\n",
    "    \"Records from the microphone and outputs the resulting data to 'path'\"\n",
    "    sample_width, data = record()\n",
    "    data = pack('<' + ('h'*len(data)), *data)\n",
    "\n",
    "    wf = wave.open(path, 'wb')\n",
    "    wf.setnchannels(1)\n",
    "    wf.setsampwidth(sample_width)\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(data)\n",
    "    wf.close()\n",
    "\n",
    "\n",
    "\n",
    "def extract_feature(file_name, **kwargs):\n",
    "    \"\"\"\n",
    "    Extract feature from audio file `file_name`\n",
    "        Features supported:\n",
    "            - MFCC (mfcc)\n",
    "            - Chroma (chroma)\n",
    "            - MEL Spectrogram Frequency (mel)\n",
    "            - Contrast (contrast)\n",
    "            - Tonnetz (tonnetz)\n",
    "        e.g:\n",
    "        `features = extract_feature(path, mel=True, mfcc=True)`\n",
    "    \"\"\"\n",
    "    mfcc = kwargs.get(\"mfcc\")\n",
    "    chroma = kwargs.get(\"chroma\")\n",
    "    mel = kwargs.get(\"mel\")\n",
    "    contrast = kwargs.get(\"contrast\")\n",
    "    tonnetz = kwargs.get(\"tonnetz\")\n",
    "    X, sample_rate = librosa.core.load(file_name)\n",
    "    if chroma or contrast:\n",
    "        stft = np.abs(librosa.stft(X))\n",
    "    result = np.array([])\n",
    "    if mfcc:\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "        result = np.hstack((result, mfccs))\n",
    "    if chroma:\n",
    "        chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "        result = np.hstack((result, chroma))\n",
    "    if mel:\n",
    "        mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
    "        result = np.hstack((result, mel))\n",
    "    if contrast:\n",
    "        contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)\n",
    "        result = np.hstack((result, contrast))\n",
    "    if tonnetz:\n",
    "        tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=sample_rate).T,axis=0)\n",
    "        result = np.hstack((result, tonnetz))\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # load the saved model (after training)\n",
    "    # model = pickle.load(open(\"result/mlp_classifier.model\", \"rb\"))\n",
    "    from utils import load_data, split_data, create_model\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description=\"\"\"Gender recognition script, this will load the model you trained, \n",
    "                                    and perform inference on a sample you provide (either using your voice or a file)\"\"\")\n",
    "    parser.add_argument(\"-f\", \"--file\", help=\"The path to the file, preferred to be in WAV format\")\n",
    "    args = parser.parse_args()\n",
    "    file = args.file\n",
    "    # construct the model\n",
    "    model = create_model()\n",
    "    # load the saved/trained weights\n",
    "    model.load_weights(\"results/model.h5\")\n",
    "    if not file or not os.path.isfile(file):\n",
    "        # if file not provided, or it doesn't exist, use your voice\n",
    "        print(\"Please talk\")\n",
    "        # put the file name here\n",
    "        file = \"test.wav\"\n",
    "        # record the file (start talking)\n",
    "        record_to_file(file)\n",
    "    # extract features and reshape it\n",
    "    features = extract_feature(file, mel=True).reshape(1, -1)\n",
    "    # predict the gender!\n",
    "    male_prob = model.predict(features)[0][0]\n",
    "    female_prob = 1 - male_prob\n",
    "    gender = \"male\" if male_prob > female_prob else \"female\"\n",
    "    # show the result!\n",
    "    print(\"Result:\", gender)\n",
    "    print(f\"Probabilities:     Male: {male_prob*100:.2f}%    Female: {female_prob*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d1f8b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
